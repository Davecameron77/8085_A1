{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76682c21-1b21-49c2-a482-3061cbb93697",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb1ffbd5-8f30-42ff-be0b-bbf4e0d41704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import Word\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e0271-b4a8-4e3b-b134-c91044a6ba15",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52efa773-1057-4c30-a5b5-bcd688670033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('yelp_academic_dataset_review.json', lines=True, nrows=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d995a67-a4ce-44cd-8460-a31931c1d565",
   "metadata": {},
   "source": [
    "## Setup Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50223167-d936-4d96-b751-6b22bc59ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'stars', 'useful', 'funny', 'cool']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01338f11-c608-4f57-940f-aefc3c684eb6",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ecf14-14e1-4290-8eb5-c21ef5c62f94",
   "metadata": {},
   "source": [
    "### Fill NA scores with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "098856f1-58ae-47d6-95c9-285fd13d1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596cce4-55f4-44bc-b333-65cc03dcf1b2",
   "metadata": {},
   "source": [
    "### Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26e9e7a1-e6b7-4ff9-a61f-d9ff93ab3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    # TODO implement this function\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "75481dd4-9936-40be-ac14-09f249467237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792988a-214a-4897-b016-4577090cc1c1",
   "metadata": {},
   "source": [
    "### Add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a307280-5b8b-4aa2-9162-4ef5573be4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['char_count'] = df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e9ec31-7a65-4b17-9c8e-a73c89514a8f",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Stop words are words deemed to add little or no value to a review. i.e. 'and.'\n",
    "Stopwords are downloaded from nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d126a-ea25-4c1b-b702-57cf85eb9d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/davecameron/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "df['stopword_count'] = df['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))\n",
    "df['stopword_rate'] = df['stopword_count'] / df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cba67-9f2e-4e55-babd-87e9b42a459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stopwords_removed'] = df['text'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2143e0f-6df8-474a-8a4b-1c0e404a208d",
   "metadata": {},
   "source": [
    "### Irrelevant words not in stop_words\n",
    "Looking for remaining words that might be considered stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b80ba-3bf2-45e0-a76f-a7d3944e802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\" \".join(df['stopwords_removed']).split()).value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ce03e-b687-4431-864a-202418ceb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stop_words = ['get', 'would', 'got', 'us', 'also', 'even', 'ive', 'im']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56845d7-8eec-444f-b18c-d4b1c532d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_reviews'] = df['stopwords_removed'].apply(lambda x: \" \".join(word for word in x.split() if word not in other_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c446b04d-5e76-4b2f-88f5-57c3375826af",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization reduces words to their root. i.e. Running is deconjucated to run, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a1ab9-a3fb-40e2-8f76-14641d1163b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized'] = df['clean_reviews'].apply(lambda x: \" \".join(Word(word).lemmatize() for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0048d-7447-4015-9f0a-bbd28d537f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_word_count'] = df['lemmatized'].apply(lambda x: len(x.split()))\n",
    "df['clean_rate'] = df['clean_word_count'] / df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c044d1b-f1dd-48c5-b659-b7cd0d983119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['word_count', \n",
    "              'char_count',\n",
    "              'stopword_count',\n",
    "              'stopword_rate',\n",
    "              'stopwords_removed',\n",
    "              'clean_reviews',\n",
    "              'clean_word_count',\n",
    "              'clean_rate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ba49b-bfb9-4bda-b80a-0eb535a986d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform_score(score):\n",
    "    return score - 1\n",
    "\n",
    "\n",
    "df['stars'] = df['stars'].apply(tranform_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dbc29f-d69e-4bff-a95f-fdd3824d2650",
   "metadata": {},
   "source": [
    "## Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f95f1e-f240-444f-9d30-47beb3723dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "compound_polarity = {}\n",
    "negativity = {}\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    negativity[i] = sia.polarity_scores(row['lemmatized'])['neg']\n",
    "    compound_polarity[i] = sia.polarity_scores(row['lemmatized'])['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0af50e-d86e-4a95-a4fe-544f72b1cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['compound_polarity'] = pd.Series(compound_polarity)\n",
    "df['negativity'] = pd.Series(negativity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78383de-9e3b-428a-9d2e-df97396c3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "down = 0\n",
    "up = 0\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if row['negativity'] > 0.1 and row['stars'] > 0:\n",
    "        row['stars'] -= 1\n",
    "        down += 1\n",
    "    if row['compound_polarity'] < 0.25 and row['stars'] > 0:\n",
    "        row['stars'] -= 1\n",
    "        down += 1\n",
    "    if row['negativity'] < 0.01 and row['stars'] < 4.0:\n",
    "        row['stars'] += 1\n",
    "        up += 1\n",
    "\n",
    "print(f'Altered {down} rows down and {up} rows up')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0206673-6c07-4c05-8371-74bc9f68d6d9",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed83baa-8802-4d66-859a-b03e52b7ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(df['text'])\n",
    "y = df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2579a3-449a-4fba-a1f1-178805e2b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b0605-db35-49df-9596-20bb73e87604",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c241ba-4cb5-48e9-9e96-9ec048945f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b3ae4-5f3f-47ce-b46c-fb1ba0e412bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553af80a-26fa-467f-bdb6-b1481b002a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
